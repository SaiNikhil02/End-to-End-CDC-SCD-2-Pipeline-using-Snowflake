Snowflake CDC & SCD Type-2 Pipeline
ğŸ“Œ Overview

This project implements an end-to-end, event-driven CDC (Change Data Capture) pipeline using Apache NiFi, Amazon S3, and Snowflake.
It demonstrates how to ingest data from external systems, apply incremental changes efficiently, and maintain SCD Type-2 history using Snowflake-native features such as Snowpipe, Streams, and Tasks.

The pipeline is designed to be scalable, fault-tolerant, replay-safe, and production-ready.

ğŸ—ï¸ Architecture

High-Level Flow
Apache NiFi (Docker on EC2)
        â†“
      Amazon S3
        â†“
   Snowpipe (Auto-Ingest)
        â†“
    customer_raw (Staging Table)
        â†“ (Snowflake Stream)
     customer (Current State)
        â†“ (Snowflake Stream)
  customer_history (SCD Type-2)

ğŸ§° Tech Stack

Apache NiFi â€“ Data ingestion and flow orchestration

Docker â€“ Containerized NiFi runtime

Amazon EC2 â€“ Compute for NiFi

Amazon S3 â€“ Durable landing zone

Snowflake

Snowpipe (auto-ingest)

Streams (CDC)

Tasks (automation)

MERGE-based incremental processing

SCD Type-2 modeling

ğŸ“‚ Repository Structure
snowflake-cdc-scd2-pipeline/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ cdc_pipeline_architecture.png
â”‚
â”œâ”€â”€ nifi/
â”‚   â”œâ”€â”€ customer_ingestion_flow.xml
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ snowflake/
â”‚   â”œâ”€â”€ ddl/
â”‚   â”‚   â”œâ”€â”€ customer_tables.sql
â”‚   â”‚   â”œâ”€â”€ streams.sql
â”‚   â”‚   â””â”€â”€ file_formats.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ snowpipe.sql
â”‚   â”‚   â””â”€â”€ copy_history_checks.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ cdc/
â”‚   â”‚   â”œâ”€â”€ merge_customer.sql
â”‚   â”‚   â””â”€â”€ scd2_customer_history.sql
â”‚   â”‚
â”‚   â””â”€â”€ tasks/
â”‚       â”œâ”€â”€ task_customer_current.sql
â”‚       â””â”€â”€ task_customer_history.sql
â”‚
â”œâ”€â”€ sample-data/
â”‚   â””â”€â”€ customer_sample.csv
â”‚
â””â”€â”€ .gitignore

<b> ğŸ”„ Data Flow Explained </b>
1ï¸âƒ£ Ingestion (Apache NiFi â†’ S3)

Apache NiFi runs on Dockerized EC2 

Ingests data from source systems

Performs light validation and routing

Writes files to Amazon S3

2ï¸âƒ£ Auto-Ingest (S3 â†’ Snowflake)

Snowpipe listens for S3 events

Automatically loads data into customer_raw

Ingestion is append-only

3ï¸âƒ£ CDC for Current State

A Snowflake Stream on customer_raw captures inserts/deletes

A MERGE operation updates the customer table:

INSERT â†’ new records

UPDATE â†’ matched records

DELETE â†’ removals

4ï¸âƒ£ SCD Type-2 History

A second Stream on customer tracks business-level changes

History logic:

Close old version (end_time, is_current = FALSE)

Insert new version (is_current = TRUE)

Ensures exactly one active row per business key

5ï¸âƒ£ Automation

Snowflake Tasks:

Triggered only when streams have data

Chained execution ensures correct ordering

No external schedulers required

ğŸ§  Key Design Decisions

Raw tables are append-only (no truncation)

One stream per consumer (best practice)

CDC is explicit, not inferred

History is driven from target table changes

Incremental processing (no full table scans)

Replay-safe and idempotent

ğŸ§ª Supported Scenarios

New customer insert

Customer updates (tracked historically)

Customer deletes (history closed)

File replays without duplicate inserts

Multiple changes per key handled safely

ğŸ” Security & Best Practices

No credentials committed to GitHub

Secrets replaced with placeholders

IAM & Snowflake RBAC enforced

Tasks run with least-privilege roles

â–¶ï¸ How to Run (High Level)

Deploy NiFi on EC2 (Docker)

Configure NiFi flow â†’ push files to S3

Create Snowflake objects:

Tables

Streams

File formats

Snowpipe

Create and resume Tasks

Monitor via:

SYSTEM$STREAM_HAS_DATA

TASK_HISTORY

COPY_HISTORY

